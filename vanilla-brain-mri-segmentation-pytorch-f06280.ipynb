{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importing the required libraries**","metadata":{}},{"cell_type":"code","source":"!pip install torchvision=='0.11.1'\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torchvision.transforms.functional as TF\n\nimport random\nimport os, shutil\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\n%matplotlib inline\n\nimport torchvision\ntorchvision.__version__","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:26:49.162944Z","iopub.execute_input":"2021-12-16T03:26:49.163294Z","iopub.status.idle":"2021-12-16T03:26:58.452671Z","shell.execute_reply.started":"2021-12-16T03:26:49.163208Z","shell.execute_reply":"2021-12-16T03:26:58.451897Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"\n# **Setup Config class**","metadata":{}},{"cell_type":"code","source":"MODEL_TYPE = \"UNET\"\n# MODEL_TYPE = \"DEFORMABLE_UNET\"\n# LOSS = \"CE\"\nLOSS = \"DICE\"\nIN_SIZE = (64, 64) #OVERRIDES THE CONFIG\n\nclass Config:   \n    # data preprocessing\n    data_dir = '/kaggle/input/lgg-mri-segmentation/kaggle_3m/'\n    logdir = 'logdir'\n    validation_fraction = 0.15\n    test_fraction = 0.10\n    train_batch = 16\n    valid_batch = 32\n    test_batch = 32\n    \n    # model setup\n    input_dim = 256\n    input_ch = 3\n    output_dim = 256\n    output_ch = 1\n    \n    # training\n    seed = 21\n    learning_rate = 3e-4\n    epochs = 30\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:26:58.454294Z","iopub.execute_input":"2021-12-16T03:26:58.454636Z","iopub.status.idle":"2021-12-16T03:26:58.485971Z","shell.execute_reply.started":"2021-12-16T03:26:58.454599Z","shell.execute_reply":"2021-12-16T03:26:58.485217Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# **Helper functions**","metadata":{}},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \ndef plot_example(idx):\n    base_path = os.path.join(Config.data_dir, test_df['directory'].iloc[idx])\n    img_path = os.path.join(base_path, test_df['images'].iloc[idx])\n    mask_path = os.path.join(base_path, test_df['masks'].iloc[idx])\n    img = Image.open(img_path)\n    mask = Image.open(mask_path)\n\n    fig, ax = plt.subplots(1, 2, figsize=(8,4))\n    ax[0].imshow(img)\n    ax[0].set_title('Image')\n    ax[1].imshow(mask) \n    ax[1].set_title('Mask')\n    plt.show()\n    \ndef show_sample(sample, title=None):\n    fig, ax = plt.subplots(1, 2)\n    ax[0].imshow(sample[0])\n    ax[1].imshow(sample[1], cmap='gray')\n    if title:\n        fig.suptitle(title)\n    plt.show()\n    \ndef plot_predictions(model, idx):\n    base_path = os.path.join(Config.data_dir, test_df['directory'].iloc[idx])\n    img_path = os.path.join(base_path, test_df['images'].iloc[idx])\n    mask_path = os.path.join(base_path, test_df['masks'].iloc[idx])\n\n    img = Image.open(img_path)\n    mask = Image.open(mask_path)\n\n    tensor_img, tensor_mask = eval_transforms((img, mask))\n    tensor_img = tensor_img.unsqueeze(0).to(Config.device) \n    \n    model.eval()\n    with torch.no_grad():\n        pred = model(tensor_img)[0].detach().cpu().numpy()\n        pred = pred.transpose((1, 2, 0)).squeeze()\n        rounded = np.round(pred)\n    \n    plot_images = {'Image': img, \n                   'Mask': mask, \n                   'Predicted Mask': pred, \n                   'Predicted Rounded Mask': rounded}\n\n    fig, ax = plt.subplots(1, 4, figsize=(16,4))\n    for i, key in enumerate(plot_images.keys()):\n        ax[i].imshow(plot_images[key])\n        ax[i].set_title(key)\n    plt.show()\n    \nset_seed(Config.seed)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-16T03:27:11.432932Z","iopub.execute_input":"2021-12-16T03:27:11.433760Z","iopub.status.idle":"2021-12-16T03:27:11.450419Z","shell.execute_reply.started":"2021-12-16T03:27:11.433722Z","shell.execute_reply":"2021-12-16T03:27:11.449618Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Loading and exploring the data**","metadata":{}},{"cell_type":"code","source":"dirs, images, masks = [], [], []\nfor root, folders, files in os.walk(Config.data_dir):\n    for file in files:\n        # save only images with corresponding masks\n        if 'mask'in file:\n            dirs.append(root.replace(Config.data_dir, ''))\n            masks.append(file)\n            images.append(file.replace('_mask', ''))\n\nPathDF = pd.DataFrame({'directory': dirs, 'images': images, 'masks': masks})\nPathDF.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:13.053021Z","iopub.execute_input":"2021-12-16T03:27:13.053640Z","iopub.status.idle":"2021-12-16T03:27:13.165711Z","shell.execute_reply.started":"2021-12-16T03:27:13.053602Z","shell.execute_reply":"2021-12-16T03:27:13.165031Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## **Train, test and validation split**","metadata":{}},{"cell_type":"code","source":"train2rest = Config.validation_fraction + Config.test_fraction\ntest2valid = Config.validation_fraction/train2rest\n\ntrain_df, rest = train_test_split(\n    PathDF, random_state=Config.seed,\n    test_size=train2rest\n)\n\ntest_df, valid_df = train_test_split(\n    rest, random_state=Config.seed,\n    test_size=test2valid\n)\n\nprint('Train:', train_df.shape[0])\nprint('Valid:', valid_df.shape[0])\nprint('Test:', test_df.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:13.933802Z","iopub.execute_input":"2021-12-16T03:27:13.934069Z","iopub.status.idle":"2021-12-16T03:27:13.946844Z","shell.execute_reply.started":"2021-12-16T03:27:13.934040Z","shell.execute_reply":"2021-12-16T03:27:13.946086Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization of the images and the masks**","metadata":{}},{"cell_type":"code","source":"plot_example(69)\nplot_example(123)\nplot_example(246)\nplot_example(346)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:14.653733Z","iopub.execute_input":"2021-12-16T03:27:14.654190Z","iopub.status.idle":"2021-12-16T03:27:16.008617Z","shell.execute_reply.started":"2021-12-16T03:27:14.654155Z","shell.execute_reply":"2021-12-16T03:27:16.007916Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset class**","metadata":{}},{"cell_type":"code","source":"class MRI_Dataset(Dataset):\n    def __init__(self, path_df, transform=None):\n        self.path_df = path_df\n        self.transform = transform\n\n    def __len__(self):\n        return self.path_df.shape[0]\n\n    def __getitem__(self, idx):\n        base_path = os.path.join(Config.data_dir, self.path_df.iloc[idx]['directory'])\n        img_path = os.path.join(base_path, self.path_df.iloc[idx]['images'])\n        mask_path = os.path.join(base_path, self.path_df.iloc[idx]['masks'])\n\n        image = Image.open(img_path)\n        mask = Image.open(mask_path)\n\n        sample = (image, mask)\n        # apply the same transform on both image and a mask\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:16.010379Z","iopub.execute_input":"2021-12-16T03:27:16.010667Z","iopub.status.idle":"2021-12-16T03:27:16.019021Z","shell.execute_reply.started":"2021-12-16T03:27:16.010629Z","shell.execute_reply":"2021-12-16T03:27:16.018164Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **Custom Transformations**","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\n\nclass PairedRandomHorizontalFlip():\n    \"\"\"Custom transform for horizontal flipping\"\"\"\n    def __init__(self, prob=0.5):\n        self.prob = prob   \n\n    def __call__(self, sample):\n        \"\"\"\n        Randomly flips both of the images\n\n        Arguments:\n        sample - tuple, image and segmentation mask\n    \n        Returns:\n        (img, mask) - tuple, transformed sample\n        \"\"\"\n        img, mask = sample\n        if np.random.random() < self.prob:\n            img, mask = TF.hflip(img), TF.hflip(mask)\n        return img, mask\n    \nclass PairedRandomAffine():\n    \"\"\"\n    Randomly applies affine transformation\n    on both of the images\n    \"\"\"\n    def __init__(self, degrees=None, translate=None, scale_ranges=None, shears=None):\n\n        self.params = {\n            'degrees': degrees,\n            'translate': translate,\n            'scale_ranges': scale_ranges,\n            'shears': shears\n        }\n\n    def __call__(self, sample):\n        img, mask = sample\n        w, h = img.size\n        # extract parameters from trasnforms.RandomAffine\n        angle, translations, scale, shear = transforms.RandomAffine.get_params(self.params['degrees'], self.params['translate'], self.params['scale_ranges'], self.params['shears'], (w,h))\n        # apply TF.affine using fixed parameters\n        img = TF.affine(img, angle, translations, scale, shear)\n        mask = TF.affine(mask, angle, translations, scale, shear)\n        return img, mask\n    \nclass Resize():\n    \"\"\"\n    Resize\n    \"\"\"\n    def __init__(self, size=IN_SIZE):\n        self.size = size\n        self.resize = torchvision.transforms.Resize(size, interpolation=Image.NEAREST)\n        \n        \n    def __call__(self, sample):\n        img, mask = sample\n        # extract parameters from trasnforms.RandomAffine\n        img = self.resize(img)\n        mask = self.resize(mask)\n        return img, mask\n    \nclass PairedToTensor():\n    \"\"\"\n    Convert ndarrays in sample to Tensors.\n    \"\"\"\n    def __call__(self, sample):\n        img, mask = sample\n        img = np.array(img)\n        mask = np.expand_dims(mask, -1)\n        img = np.moveaxis(img, -1, 0)\n        mask = np.moveaxis(mask, -1, 0)\n        img, mask =  torch.FloatTensor(img), torch.FloatTensor(mask)\n        img = img/255\n        mask = mask/255\n        return img, mask\n    \ndataset = MRI_Dataset(test_df)\nsample = dataset[59]\ntransform = PairedRandomHorizontalFlip(prob=1)\nshow_sample(sample, title='Original')\nshow_sample(transform(sample), title='Transformed')","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:16.277874Z","iopub.execute_input":"2021-12-16T03:27:16.278130Z","iopub.status.idle":"2021-12-16T03:27:16.806433Z","shell.execute_reply.started":"2021-12-16T03:27:16.278083Z","shell.execute_reply":"2021-12-16T03:27:16.805573Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"transform = PairedRandomAffine(\n    degrees=(15, 15),\n    scale_ranges=(1.2, 1.2)\n) # for testing purposes we fix degrees and scale to a constant number\nshow_sample(sample, 'Original')\nshow_sample(transform(sample), 'Transformed')","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:16.808030Z","iopub.execute_input":"2021-12-16T03:27:16.808369Z","iopub.status.idle":"2021-12-16T03:27:17.343608Z","shell.execute_reply.started":"2021-12-16T03:27:16.808332Z","shell.execute_reply":"2021-12-16T03:27:17.342951Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# **Data transformation and loaders**","metadata":{}},{"cell_type":"code","source":"train_transforms = transforms.Compose([                       \n    PairedRandomHorizontalFlip(), \n    PairedRandomAffine(\n        degrees=(-15, 15),\n        translate=(0.1, 0.1),\n        scale_ranges=(0.8, 1.2)\n    ),\n    Resize(IN_SIZE),\n    PairedToTensor()\n])\neval_transforms = transforms.Compose([                       \n    Resize(IN_SIZE),\n    PairedToTensor()\n])\n\ntrain_data = MRI_Dataset(train_df, transform=train_transforms)\nvalid_data = MRI_Dataset(valid_df, transform=eval_transforms)\ntest_data = MRI_Dataset(test_df, transform=eval_transforms)\n\ntrain_loader = DataLoader(train_data, batch_size=Config.train_batch, shuffle=True, num_workers=2)\nvalid_loader = DataLoader(valid_data, batch_size=Config.valid_batch, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_data, batch_size=Config.test_batch, shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:17.656747Z","iopub.execute_input":"2021-12-16T03:27:17.657666Z","iopub.status.idle":"2021-12-16T03:27:17.669652Z","shell.execute_reply.started":"2021-12-16T03:27:17.657615Z","shell.execute_reply":"2021-12-16T03:27:17.668807Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# **Define the model**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.ops\nfrom torch import nn\n\nclass DeformableConv2d(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 stride=1,\n                 padding=1,\n                 bias=False):\n\n        super(DeformableConv2d, self).__init__()\n        \n        assert type(kernel_size) == tuple or type(kernel_size) == int\n\n        kernel_size = kernel_size if type(kernel_size) == tuple else (kernel_size, kernel_size)\n        self.stride = stride if type(stride) == tuple else (stride, stride)\n        self.padding = padding\n        \n        self.offset_conv = nn.Conv2d(in_channels, \n                                     2 * kernel_size[0] * kernel_size[1],\n                                     kernel_size=kernel_size, \n                                     stride=stride,\n                                     padding=self.padding, \n                                     bias=True)\n\n        nn.init.constant_(self.offset_conv.weight, 0.)\n        nn.init.constant_(self.offset_conv.bias, 0.)\n        \n        self.modulator_conv = nn.Conv2d(in_channels, \n                                     1 * kernel_size[0] * kernel_size[1],\n                                     kernel_size=kernel_size, \n                                     stride=stride,\n                                     padding=self.padding, \n                                     bias=True)\n\n        nn.init.constant_(self.modulator_conv.weight, 0.)\n        nn.init.constant_(self.modulator_conv.bias, 0.)\n        \n        self.regular_conv = nn.Conv2d(in_channels=in_channels,\n                                      out_channels=out_channels,\n                                      kernel_size=kernel_size,\n                                      stride=stride,\n                                      padding=self.padding,\n                                      bias=bias)\n\n    def forward(self, x):\n        #h, w = x.shape[2:]\n        #max_offset = max(h, w)/4.\n\n        offset = self.offset_conv(x)#.clamp(-max_offset, max_offset)\n        modulator = 2. * torch.sigmoid(self.modulator_conv(x))\n        \n        x = torchvision.ops.deform_conv2d(input=x, \n                                          offset=offset, \n                                          weight=self.regular_conv.weight, \n                                          bias=self.regular_conv.bias, \n                                          padding=self.padding,\n                                          mask=modulator,\n                                          stride=self.stride,\n                                          )\n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:18.949357Z","iopub.execute_input":"2021-12-16T03:27:18.949696Z","iopub.status.idle":"2021-12-16T03:27:18.962211Z","shell.execute_reply.started":"2021-12-16T03:27:18.949660Z","shell.execute_reply":"2021-12-16T03:27:18.961169Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_ch, out_ch, conv=nn.Conv2d):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential( \n            conv(in_ch, out_ch, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            conv(out_ch, out_ch, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n         )\n    def forward(self, x):\n        x = self.conv(x)\n#         print(\"Double conv output: \", x.shape)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:21.882025Z","iopub.execute_input":"2021-12-16T03:27:21.882987Z","iopub.status.idle":"2021-12-16T03:27:21.889977Z","shell.execute_reply.started":"2021-12-16T03:27:21.882933Z","shell.execute_reply":"2021-12-16T03:27:21.889045Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class InConv(nn.Module):\n    def __init__(self, in_ch, out_ch, conv=nn.Conv2d):\n        super(InConv, self).__init__()\n        self.conv = DoubleConv(in_ch, out_ch, conv=conv)\n    def forward(self, x):\n        x = self.conv(x)\n#         print(\"InConv conv output: \", x.shape)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:22.410647Z","iopub.execute_input":"2021-12-16T03:27:22.411407Z","iopub.status.idle":"2021-12-16T03:27:22.416334Z","shell.execute_reply.started":"2021-12-16T03:27:22.411364Z","shell.execute_reply":"2021-12-16T03:27:22.415458Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Down(nn.Module):\n    def __init__(self, in_ch, out_ch, conv=nn.Conv2d):\n        super(Down, self).__init__()\n        self.mpconv = nn.Sequential( \n            nn.MaxPool2d(2,2),\n            DoubleConv(in_ch, out_ch, conv=conv)\n         )\n    def forward(self, x):\n        x = self.mpconv(x)\n#         print(\"Down conv output: \", x.shape)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:23.018023Z","iopub.execute_input":"2021-12-16T03:27:23.018687Z","iopub.status.idle":"2021-12-16T03:27:23.025464Z","shell.execute_reply.started":"2021-12-16T03:27:23.018646Z","shell.execute_reply":"2021-12-16T03:27:23.024151Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Up(nn.Module):\n    def __init__(self, in_ch, out_ch, conv=nn.Conv2d):\n        super(Up, self).__init__()\n        self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, kernel_size=2, stride=2)\n        self.conv = DoubleConv(in_ch, out_ch, conv=conv)\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n#         print(\"Up conv output: \", x.shape)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:23.405915Z","iopub.execute_input":"2021-12-16T03:27:23.406194Z","iopub.status.idle":"2021-12-16T03:27:23.412548Z","shell.execute_reply.started":"2021-12-16T03:27:23.406163Z","shell.execute_reply":"2021-12-16T03:27:23.411670Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# class OutConv(nn.Module):\n#     def __init__(self, in_ch, out_ch, conv=nn.Conv2d):\n#         super(OutConv, self).__init__()\n#         self.conv = conv(in_ch, out_ch, 1, padding=0)\n#         self.sigmoid = nn.Sigmoid()\n#     def forward(self, x):\n#         x = self.conv(x)\n#         x = self.sigmoid(x)\n# #         print(\"OutConv conv output: \", x.shape)\n#         return x\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels, conv=nn.Conv2d):\n        super(OutConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n            nn.Sigmoid())\n    def forward(self, x):\n        return self.conv(x)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:23.825289Z","iopub.execute_input":"2021-12-16T03:27:23.825957Z","iopub.status.idle":"2021-12-16T03:27:23.831870Z","shell.execute_reply.started":"2021-12-16T03:27:23.825919Z","shell.execute_reply":"2021-12-16T03:27:23.831012Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, in_channels, num_classes, conv=nn.Conv2d):\n        super(UNet, self).__init__()\n        self.inc = InConv(in_channels, 64, conv=conv)\n        self.down1 = Down(64, 128, conv=conv)\n        self.down2 = Down(128, 256, conv=conv)\n        self.down3 = Down(256, 512, conv=conv)\n        self.down4 = Down(512, 512, conv=conv)\n        self.up1 = Up(1024, 256, conv=conv)\n        self.up2 = Up(512, 128, conv=conv)\n        self.up3 = Up(256, 64, conv=conv)\n        self.up4 = Up(128, 64, conv=conv)\n        self.outc = OutConv(64, num_classes, conv=conv)\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = self.outc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:24.500291Z","iopub.execute_input":"2021-12-16T03:27:24.500872Z","iopub.status.idle":"2021-12-16T03:27:24.513093Z","shell.execute_reply.started":"2021-12-16T03:27:24.500833Z","shell.execute_reply":"2021-12-16T03:27:24.512445Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# **Train loop**","metadata":{}},{"cell_type":"code","source":"def train_loop(model, optimizer, criterion, train_loader, device=Config.device):\n    running_loss = 0\n    model.train()\n    pbar = tqdm(train_loader, desc='Iterating over train data')\n    for imgs, masks in pbar:\n        # pass to device\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        # forward\n        out = model(imgs)\n#         print(\"imgs: \", imgs.shape)\n#         print(\"masks: \", masks.shape)\n#         print(\"out: \", out.shape)\n        \n        loss = criterion(out, masks)\n        running_loss += loss.item()*imgs.shape[0] \n        # optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    running_loss /= len(train_loader.sampler)\n    return running_loss","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:27.869801Z","iopub.execute_input":"2021-12-16T03:27:27.870060Z","iopub.status.idle":"2021-12-16T03:27:27.878583Z","shell.execute_reply.started":"2021-12-16T03:27:27.870032Z","shell.execute_reply":"2021-12-16T03:27:27.877847Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# **Evaluation loop**","metadata":{}},{"cell_type":"code","source":"def dice_coef_metric(inputs, target):\n    intersection = 2.0 * (target * inputs).sum()\n    union = target.sum() + inputs.sum()\n    if target.sum() == 0 and inputs.sum() == 0:\n        return 1.0\n    return intersection / union\n\n\ndef iou_metric(inputs, target):\n    intersection = (target * inputs).sum()\n    union = target.sum() + inputs.sum()\n    if target.sum() == 0 and inputs.sum() == 0:\n        return 1.0\n    return intersection / union","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:28.842391Z","iopub.execute_input":"2021-12-16T03:27:28.842677Z","iopub.status.idle":"2021-12-16T03:27:28.850727Z","shell.execute_reply.started":"2021-12-16T03:27:28.842646Z","shell.execute_reply":"2021-12-16T03:27:28.849903Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def eval_loop(model, criterion, eval_loader, device=Config.device):\n    global labelsx\n    running_loss = 0\n    model.eval()\n    with torch.no_grad():\n        accuracy, f1_scores = [], []\n        dices, ious = [], []\n        pbar = tqdm(eval_loader, desc='Iterating over evaluation data')\n        for imgs, masks in pbar:\n            # pass to device\n            imgs = imgs.to(device)\n            masks = masks.to(device)\n            # forward\n            out = model(imgs)\n            loss = criterion(out, masks)\n            running_loss += loss.item()*imgs.shape[0]\n            # calculate predictions using output\n            predicted = (out > 0.5).float()\n            predicted = predicted.view(-1).cpu().numpy()\n            labels = masks.view(-1).cpu().numpy()\n            accuracy.append(accuracy_score(labels, predicted))\n            f1_scores.append(f1_score(labels, predicted))\n            dices.append(dice_coef_metric(labels, predicted))\n            ious.append(iou_metric(labels, predicted))\n    acc = sum(accuracy)/len(accuracy)\n    f1 = sum(f1_scores)/len(f1_scores)\n    dice = sum(dices)/len(dices)\n    iou = sum(ious)/len(ious)\n    running_loss /= len(eval_loader.sampler)\n    return {\n        'accuracy':acc,\n        'f1_macro':f1, \n        'loss':running_loss,\n        'dice': dice,\n        'iou': iou,\n    }","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:29.381730Z","iopub.execute_input":"2021-12-16T03:27:29.382378Z","iopub.status.idle":"2021-12-16T03:27:29.393442Z","shell.execute_reply.started":"2021-12-16T03:27:29.382337Z","shell.execute_reply":"2021-12-16T03:27:29.391333Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# **Train the model**","metadata":{}},{"cell_type":"code","source":"def train(model, optimizer, criterion, train_loader, valid_loader,\n          device=Config.device, \n          num_epochs=Config.epochs, \n          valid_loss_min=np.inf):\n    \n    for e in range(num_epochs):\n        # train for epoch\n        train_loss = train_loop(\n            model, optimizer, criterion, train_loader, device=device)\n        # evaluate on validation set\n        metrics = eval_loop(\n            model, criterion, valid_loader, device=device\n        )\n        # show progress\n        print_string = f'Epoch: {e+1} '\n        print_string+= f'TrainLoss: {train_loss:.5f} '\n        print_string+= f'ValidLoss: {metrics[\"loss\"]:.5f} '\n        print_string+= f'ACC: {metrics[\"accuracy\"]:.5f} '\n        print_string+= f'F1: {metrics[\"f1_macro\"]:.3f}'\n        print_string+= f' Dice: {metrics[\"dice\"]:.3f}'\n        print_string+= f' IoU: {metrics[\"iou\"]:.3f}'\n        print(print_string)\n\n        # save the model \n        if metrics[\"loss\"] <= valid_loss_min:\n            torch.save(model.state_dict(), 'UNet.pt')\n            valid_loss_min = metrics[\"loss\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:30.244816Z","iopub.execute_input":"2021-12-16T03:27:30.245126Z","iopub.status.idle":"2021-12-16T03:27:30.252274Z","shell.execute_reply.started":"2021-12-16T03:27:30.245070Z","shell.execute_reply":"2021-12-16T03:27:30.251259Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def dice(input, target):\n    smooth = 1.\n\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n    intersection = (iflat * tflat).sum()\n    \n    return 1 - ((2. * intersection + smooth) /\n              (iflat.sum() + tflat.sum() + smooth))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:31.028595Z","iopub.execute_input":"2021-12-16T03:27:31.029467Z","iopub.status.idle":"2021-12-16T03:27:31.035523Z","shell.execute_reply.started":"2021-12-16T03:27:31.029413Z","shell.execute_reply":"2021-12-16T03:27:31.034568Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"set_seed(Config.seed)\nif MODEL_TYPE == \"UNET\":\n    model = UNet(Config.input_ch, Config.output_ch).to(Config.device)\nif MODEL_TYPE == \"DEFORMABLE_UNET\":\n    model = UNet(Config.input_ch, Config.output_ch, conv=DeformableConv2d).to(Config.device)\noptimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\nif LOSS == \"CE\":\n    criterion = nn.BCELoss()\nelse:\n    criterion = dice","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:39.733767Z","iopub.execute_input":"2021-12-16T03:27:39.734031Z","iopub.status.idle":"2021-12-16T03:27:42.502173Z","shell.execute_reply.started":"2021-12-16T03:27:39.734000Z","shell.execute_reply":"2021-12-16T03:27:42.501412Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def get_n_params(model):\n    pp=0\n    for p in list(model.parameters()):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:42.503906Z","iopub.execute_input":"2021-12-16T03:27:42.504177Z","iopub.status.idle":"2021-12-16T03:27:42.510513Z","shell.execute_reply.started":"2021-12-16T03:27:42.504142Z","shell.execute_reply":"2021-12-16T03:27:42.509769Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"get_n_params(model)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:42.511728Z","iopub.execute_input":"2021-12-16T03:27:42.512139Z","iopub.status.idle":"2021-12-16T03:27:42.520784Z","shell.execute_reply.started":"2021-12-16T03:27:42.512082Z","shell.execute_reply":"2021-12-16T03:27:42.520108Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train(model, optimizer, criterion, train_loader, valid_loader)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:27:43.370281Z","iopub.execute_input":"2021-12-16T03:27:43.371160Z","iopub.status.idle":"2021-12-16T03:28:11.729387Z","shell.execute_reply.started":"2021-12-16T03:27:43.371088Z","shell.execute_reply":"2021-12-16T03:28:11.728075Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Load the latest model\nmodel.load_state_dict(torch.load('UNet.pt'))\nmetrics = eval_loop(model, criterion, train_loader)\nprint(\"SEARCH THIS STRING SEARCH SERCH SEARCH\")\n\n\nprint('Train accuracy:', metrics['accuracy'])\nprint('Train f1 macro:', metrics['f1_macro'])\nprint('Train loss:', metrics['loss'])\nprint('Train IoU:', metrics['iou'])\nprint('Train Dice:', metrics['dice'])\n\nmetrics = eval_loop(model, criterion, test_loader)\nprint('Test accuracy:', metrics['accuracy'])\nprint('Test f1 macro:', metrics['f1_macro'])\nprint('Test loss:', metrics['loss'])\nprint('Test IoU:', metrics['iou'])\nprint('Test Dice:', metrics['dice'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualizing the results**","metadata":{}},{"cell_type":"code","source":"plot_predictions(model, 69)\nplot_predictions(model, 123)\nplot_predictions(model, 246)\nplot_predictions(model, 346)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(50,70):\n    plot_predictions(model, i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}